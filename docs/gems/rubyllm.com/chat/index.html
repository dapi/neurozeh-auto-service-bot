<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="../assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="../assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul:nth-of-type(4) > li:not(:nth-child(1)) > a, .site-nav > ul:nth-of-type(4) > li > ul > li a { background-image: none; } .site-nav > ul:not(:nth-of-type(4)) a, .site-nav li.external a { background-image: none; } .site-nav > ul:nth-of-type(4) > li:nth-child(1) > a { font-weight: 600; text-decoration: none; } .site-nav > ul.nav-category-list > li > button svg, .site-nav > ul:nth-of-type(4) > li:nth-child(1) > button svg { transform: rotate(-90deg); } .site-nav > ul.nav-category-list > li.nav-list-item > ul.nav-list, .site-nav > ul:nth-of-type(4) > li.nav-list-item:nth-child(1) > ul.nav-list { display: block; } </style> <script src="../assets/js/vendor/lunr.min.js"></script> <script src="../assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <link rel="icon" href="../assets/images/logo.svg" type="image/x-icon"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>Chat | RubyLLM</title> <meta name="generator" content="Jekyll v4.4.1" /> <meta property="og:title" content="Chat" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="Learn how to have conversations with AI models, work with different providers, and handle multi-modal inputs" /> <meta property="og:description" content="Learn how to have conversations with AI models, work with different providers, and handle multi-modal inputs" /> <link rel="canonical" href="index.html" /> <meta property="og:url" content="https://rubyllm.com/chat/" /> <meta property="og:site_name" content="RubyLLM" /> <meta property="og:image" content="https://rubyllm.com/assets/images/og/core_features/chat.png" /> <meta property="og:image:height" content="600" /> <meta property="og:image:width" content="1200" /> <meta property="og:image:alt" content="Chat" /> <meta property="og:type" content="article" /> <meta property="article:published_time" content="2025-10-22T09:47:33+00:00" /> <meta name="twitter:card" content="summary_large_image" /> <meta property="twitter:image" content="https://rubyllm.com/assets/images/og/core_features/chat.png" /> <meta name="twitter:image:alt" content="Chat" /> <meta property="twitter:title" content="Chat" /> <meta name="twitter:site" content="@paolino" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-10-22T09:47:33+00:00","datePublished":"2025-10-22T09:47:33+00:00","description":"Learn how to have conversations with AI models, work with different providers, and handle multi-modal inputs","headline":"Chat","image":{"width":1200,"height":600,"alt":"Chat","url":"https://rubyllm.com/assets/images/og/core_features/chat.png","@type":"imageObject"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://rubyllm.com/chat/"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://rubyllm.com/assets/images/logotype.svg"}},"url":"https://rubyllm.com/chat/"}</script> <!-- End Jekyll SEO tag --> <!-- Auto Dark Mode - Must run immediately to prevent flash --> <script src="../assets/js/auto-dark-mode.js"></script> <link rel="icon" type="image/png" href="../assets/images/favicon/favicon-96x96.png" sizes="96x96" /> <link rel="icon" type="image/svg+xml" href="../assets/images/logo.svg" /> <link rel="shortcut icon" href="../assets/images/favicon/favicon.ico" /> <link rel="apple-touch-icon" sizes="180x180" href="../assets/images/favicon/apple-touch-icon.png" /> <meta name="apple-mobile-web-app-title" content="RubyLLM" /> <link rel="manifest" href="../assets/images/favicon/site.webmanifest" /> <!-- Override Twitter card type to use large image --> <meta name="twitter:card" content="summary_large_image"> <script defer data-domain="rubyllm.com" src="https://stats.paolino.me/js/script.outbound-links.js"></script> </head> <body> <a class="skip-to-main" href="index.html#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="../index.html" class="site-title lh-tight"> <div class="site-logo" role="img" aria-label="RubyLLM"></div> </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="../index.html" class="nav-list-link">Home</a></li></ul> <ul class="nav-list"><li class="nav-list-item external"> <a href="https://github.com/crmne/ruby_llm" class="nav-list-link external" > GitHub <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg> </a> </li><li class="nav-list-item external"> <a href="https://github.com/crmne/ruby_llm/releases" class="nav-list-link external" > Changelog <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg> </a> </li><li class="nav-list-item external"> <a href="https://paolino.me" class="nav-list-link external" > Blog <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg> </a> </li></ul> <div class="nav-category">Getting Started</div> <ul class="nav-list"><li class="nav-list-item"><a href="../getting-started/index.html" class="nav-list-link">Getting Started</a></li><li class="nav-list-item"><a href="../overview/index.html" class="nav-list-link">Overview</a></li><li class="nav-list-item"><a href="../configuration/index.html" class="nav-list-link">Configuration</a></li></ul> <div class="nav-category">Core Features</div> <ul class="nav-list"><li class="nav-list-item"><a href="index.html" class="nav-list-link">Chat</a></li><li class="nav-list-item"><a href="../tools/index.html" class="nav-list-link">Tools</a></li><li class="nav-list-item"><a href="../streaming/index.html" class="nav-list-link">Stream Responses</a></li><li class="nav-list-item"><a href="../embeddings/index.html" class="nav-list-link">Embeddings</a></li><li class="nav-list-item"><a href="../image-generation/index.html" class="nav-list-link">Image Generation</a></li><li class="nav-list-item"><a href="../audio-transcription/index.html" class="nav-list-link">Audio Transcription</a></li><li class="nav-list-item"><a href="../moderation/index.html" class="nav-list-link">Moderation</a></li></ul> <div class="nav-category">Advanced</div> <ul class="nav-list"><li class="nav-list-item"><a href="../rails/index.html" class="nav-list-link">Rails Integration</a></li><li class="nav-list-item"><a href="../async/index.html" class="nav-list-link">Scale with Async</a></li><li class="nav-list-item"><a href="../error-handling/index.html" class="nav-list-link">Error Handling</a></li><li class="nav-list-item"><a href="../models/index.html" class="nav-list-link">Model Registry</a></li><li class="nav-list-item"><a href="../agentic-workflows/index.html" class="nav-list-link">Agentic Workflows</a></li><li class="nav-list-item"><a href="../upgrading/index.html" class="nav-list-link">Upgrading</a></li></ul> <div class="nav-category">Reference</div> <ul class="nav-list"><li class="nav-list-item"><a href="../available-models/index.html" class="nav-list-link">Available Models</a></li><li class="nav-list-item"><a href="../ecosystem/index.html" class="nav-list-link">RubyLLM Ecosystem</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search RubyLLM" aria-label="Search RubyLLM" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 class="no_toc" id="chat"> <a href="index.html#chat" class="anchor-heading" aria-labelledby="chat"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chat </h1> <p class="fs-6 fw-300">Learn how to have conversations with AI models, work with different providers, and handle multi-modal inputs</p> <h2 class="no_toc text-delta" id="table-of-contents"> <a href="index.html#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of contents </h2> <ol id="markdown-toc"> <li><a href="index.html#starting-a-conversation" id="markdown-toc-starting-a-conversation">Starting a Conversation</a></li> <li><a href="index.html#continuing-the-conversation" id="markdown-toc-continuing-the-conversation">Continuing the Conversation</a></li> <li><a href="index.html#guiding-ai-behavior-with-system-prompts" id="markdown-toc-guiding-ai-behavior-with-system-prompts">Guiding AI Behavior with System Prompts</a></li> <li><a href="index.html#working-with-different-models" id="markdown-toc-working-with-different-models">Working with Different Models</a></li> <li><a href="index.html#multi-modal-conversations" id="markdown-toc-multi-modal-conversations">Multi-modal Conversations</a> <ol> <li><a href="index.html#working-with-images" id="markdown-toc-working-with-images">Working with Images</a></li> <li><a href="index.html#working-with-videos" id="markdown-toc-working-with-videos">Working with Videos</a></li> <li><a href="index.html#working-with-audio" id="markdown-toc-working-with-audio">Working with Audio</a></li> <li><a href="index.html#working-with-text-files" id="markdown-toc-working-with-text-files">Working with Text Files</a></li> <li><a href="index.html#working-with-pdfs" id="markdown-toc-working-with-pdfs">Working with PDFs</a></li> <li><a href="index.html#automatic-file-type-detection" id="markdown-toc-automatic-file-type-detection">Automatic File Type Detection</a></li> </ol> </li> <li><a href="index.html#controlling-response-behavior" id="markdown-toc-controlling-response-behavior">Controlling Response Behavior</a> <ol> <li><a href="index.html#temperature-and-creativity" id="markdown-toc-temperature-and-creativity">Temperature and Creativity</a></li> <li><a href="index.html#provider-specific-parameters" id="markdown-toc-provider-specific-parameters">Provider-Specific Parameters</a></li> </ol> </li> <li><a href="index.html#raw-content-blocks" id="markdown-toc-raw-content-blocks">Raw Content Blocks</a> <ol> <li><a href="index.html#anthropic-prompt-caching" id="markdown-toc-anthropic-prompt-caching">Anthropic Prompt Caching</a></li> <li><a href="index.html#custom-http-headers" id="markdown-toc-custom-http-headers">Custom HTTP Headers</a></li> </ol> </li> <li><a href="index.html#getting-structured-output" id="markdown-toc-getting-structured-output">Getting Structured Output</a> <ol> <li><a href="index.html#using-rubyllmschema-recommended" id="markdown-toc-using-rubyllmschema-recommended">Using RubyLLM::Schema (Recommended)</a></li> <li><a href="index.html#using-manual-json-schemas" id="markdown-toc-using-manual-json-schemas">Using Manual JSON Schemas</a></li> <li><a href="index.html#complex-nested-schemas" id="markdown-toc-complex-nested-schemas">Complex Nested Schemas</a></li> <li><a href="index.html#provider-support" id="markdown-toc-provider-support">Provider Support</a></li> <li><a href="index.html#multi-turn-conversations-with-schemas" id="markdown-toc-multi-turn-conversations-with-schemas">Multi-turn Conversations with Schemas</a></li> </ol> </li> <li><a href="index.html#tracking-token-usage" id="markdown-toc-tracking-token-usage">Tracking Token Usage</a></li> <li><a href="index.html#chat-event-handlers" id="markdown-toc-chat-event-handlers">Chat Event Handlers</a> <ol> <li><a href="index.html#available-event-handlers" id="markdown-toc-available-event-handlers">Available Event Handlers</a></li> </ol> </li> <li><a href="index.html#raw-responses" id="markdown-toc-raw-responses">Raw Responses</a></li> <li><a href="index.html#next-steps" id="markdown-toc-next-steps">Next Steps</a></li> </ol><hr /> <p>After reading this guide, you will know:</p> <ul> <li>How to start and continue conversations with AI models</li> <li>How to select and work with different models and providers</li> <li>How to guide AI behavior with system prompts</li> <li>How to work with images, audio, documents, and other file types</li> <li>How to control response creativity and format</li> <li>How to get structured output with JSON schemas</li> <li>How to track token usage and costs</li> <li>How to handle streaming responses and events</li> </ul> <h2 id="starting-a-conversation"> <a href="index.html#starting-a-conversation" class="anchor-heading" aria-labelledby="starting-a-conversation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Starting a Conversation </h2> <p>When you want to interact with an AI model, you create a chat instance. The simplest approach uses <code class="language-plaintext highlighter-rouge">RubyLLM.chat</code>, which creates a new conversation with your configured default model.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>

<span class="c1"># The ask method sends a user message and returns the assistant's response</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Explain the concept of 'Convention over Configuration' in Rails."</span>

<span class="c1"># The response is a RubyLLM::Message object</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
<span class="c1"># =&gt; "Convention over Configuration (CoC) is a core principle of Ruby on Rails..."</span>

<span class="c1"># The response object contains metadata</span>
<span class="nb">puts</span> <span class="s2">"Model Used: </span><span class="si">#{</span><span class="n">response</span><span class="p">.</span><span class="nf">model_id</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">puts</span> <span class="s2">"Tokens Used: </span><span class="si">#{</span><span class="n">response</span><span class="p">.</span><span class="nf">input_tokens</span><span class="si">}</span><span class="s2"> input, </span><span class="si">#{</span><span class="n">response</span><span class="p">.</span><span class="nf">output_tokens</span><span class="si">}</span><span class="s2"> output"</span>
<span class="nb">puts</span> <span class="s2">"Cached Prompt Tokens: </span><span class="si">#{</span><span class="n">response</span><span class="p">.</span><span class="nf">cached_tokens</span><span class="si">}</span><span class="s2">"</span> <span class="c1"># v1.9.0+</span>
<span class="nb">puts</span> <span class="s2">"Cache Writes: </span><span class="si">#{</span><span class="n">response</span><span class="p">.</span><span class="nf">cache_creation_tokens</span><span class="si">}</span><span class="s2">"</span> <span class="c1"># v1.9.0+</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">ask</code> method adds your message to the conversation history with the <code class="language-plaintext highlighter-rouge">:user</code> role, sends the entire conversation history to the AI provider, and returns a <code class="language-plaintext highlighter-rouge">RubyLLM::Message</code> object containing the assistant’s response.</p> <p>The <code class="language-plaintext highlighter-rouge">say</code> method is an alias for <code class="language-plaintext highlighter-rouge">ask</code>, so you can use whichever feels more natural in your code.</p> <h2 id="continuing-the-conversation"> <a href="index.html#continuing-the-conversation" class="anchor-heading" aria-labelledby="continuing-the-conversation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Continuing the Conversation </h2> <p>One of the key features of chat-based AI models is their ability to maintain context across multiple exchanges. The <code class="language-plaintext highlighter-rouge">Chat</code> object automatically manages this conversation history for you.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Continuing the previous chat...</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Can you give a specific example in Rails?"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
<span class="c1"># =&gt; "Certainly! A classic example is database table naming..."</span>

<span class="c1"># Access the full conversation history</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">messages</span><span class="p">.</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">message</span><span class="o">|</span>
  <span class="nb">puts</span> <span class="s2">"[</span><span class="si">#{</span><span class="n">message</span><span class="p">.</span><span class="nf">role</span><span class="p">.</span><span class="nf">to_s</span><span class="p">.</span><span class="nf">upcase</span><span class="si">}</span><span class="s2">] </span><span class="si">#{</span><span class="n">message</span><span class="p">.</span><span class="nf">content</span><span class="p">.</span><span class="nf">lines</span><span class="p">.</span><span class="nf">first</span><span class="p">.</span><span class="nf">strip</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
<span class="c1"># =&gt; [USER] Explain the concept of 'Convention over Configuration' in Rails.</span>
<span class="c1"># =&gt; [ASSISTANT] Convention over Configuration (CoC) is a core principle...</span>
<span class="c1"># =&gt; [USER] Can you give a specific example in Rails?</span>
<span class="c1"># =&gt; [ASSISTANT] Certainly! A classic example is database table naming...</span>
</code></pre></div></div> <p>Each time you call <code class="language-plaintext highlighter-rouge">ask</code>, RubyLLM sends the entire conversation history to the AI provider. This allows the model to understand the full context of your conversation, enabling natural follow-up questions and maintaining coherent dialogue.</p> <h2 id="guiding-ai-behavior-with-system-prompts"> <a href="index.html#guiding-ai-behavior-with-system-prompts" class="anchor-heading" aria-labelledby="guiding-ai-behavior-with-system-prompts"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Guiding AI Behavior with System Prompts </h2> <p>System prompts, also called instructions, allow you to set the overall behavior, personality, and constraints for the AI assistant. These instructions persist throughout the conversation and help ensure consistent responses.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>

<span class="c1"># Set the initial instruction</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">with_instructions</span> <span class="s2">"You are a helpful assistant that explains Ruby concepts simply, like explaining to a five-year-old."</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What is a variable?"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
<span class="c1"># =&gt; "Imagine you have a special box, and you can put things in it..."</span>

<span class="c1"># Use replace: true to ensure only the latest instruction is active</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">with_instructions</span> <span class="s2">"Always end your response with 'Got it?'"</span><span class="p">,</span> <span class="ss">replace: </span><span class="kp">true</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What is a loop?"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
<span class="c1"># =&gt; "A loop is like singing your favorite song over and over again... Got it?"</span>
</code></pre></div></div> <p>System prompts are added to the conversation as messages with the <code class="language-plaintext highlighter-rouge">:system</code> role and are sent with every request to the AI provider. This ensures the model always considers your instructions when generating responses.</p> <blockquote class="note"> <p>When using the <a href="../rails/index.html">Rails Integration</a>, system messages are persisted in your database along with user and assistant messages, maintaining the full conversation context.</p> </blockquote> <h2 id="working-with-different-models"> <a href="index.html#working-with-different-models" class="anchor-heading" aria-labelledby="working-with-different-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Working with Different Models </h2> <p>RubyLLM supports over 600 models from various providers. While <code class="language-plaintext highlighter-rouge">RubyLLM.chat</code> uses your configured default model, you can specify different models:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use a specific model via ID or alias</span>
<span class="n">chat_claude</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>
<span class="n">chat_gemini</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gemini-2.5-pro'</span><span class="p">)</span>

<span class="c1"># Change the model on an existing chat instance</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gpt-4.1-nano'</span><span class="p">)</span>
<span class="n">response1</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Initial question..."</span>

<span class="n">chat</span><span class="p">.</span><span class="nf">with_model</span><span class="p">(</span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>
<span class="n">response2</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Follow-up question..."</span>
</code></pre></div></div> <p>For detailed information about model selection, capabilities, aliases, and working with custom models, see the <a href="../models/index.html">Working with Models Guide</a>.</p> <h2 id="multi-modal-conversations"> <a href="index.html#multi-modal-conversations" class="anchor-heading" aria-labelledby="multi-modal-conversations"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multi-modal Conversations </h2> <p>Many modern AI models can process multiple types of input beyond just text. RubyLLM provides a unified interface for working with images, audio, documents, and other file types through the <code class="language-plaintext highlighter-rouge">with:</code> parameter.</p> <h3 id="working-with-images"> <a href="index.html#working-with-images" class="anchor-heading" aria-labelledby="working-with-images"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Working with Images </h3> <p>Vision-capable models can analyze images, answer questions about visual content, and even compare multiple images.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Ensure you select a vision-capable model</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gpt-4.1'</span><span class="p">)</span>

<span class="c1"># Ask about a local image file</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Describe this logo."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/ruby_logo.png"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Ask about an image from a URL</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What kind of architecture is shown here?"</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"https://example.com/eiffel_tower.jpg"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Send multiple images</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Compare the user interfaces in these two screenshots."</span><span class="p">,</span> <span class="ss">with: </span><span class="p">[</span><span class="s2">"screenshot_v1.png"</span><span class="p">,</span> <span class="s2">"screenshot_v2.png"</span><span class="p">]</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <h3 id="working-with-videos"> <a href="index.html#working-with-videos" class="anchor-heading" aria-labelledby="working-with-videos"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Working with Videos </h3> <p>You can also analyze video files or URLs with video-capable models. RubyLLM will automatically detect video files and handle them appropriately.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Ask about a local video file</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gemini-2.5-flash'</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What happens in this video?"</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/demo.mp4"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Ask about a video from a URL</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Summarize the main events in this video."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"https://example.com/demo_video.mp4"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Combine videos with other file types</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Analyze these files for visual content."</span><span class="p">,</span> <span class="ss">with: </span><span class="p">[</span><span class="s2">"diagram.png"</span><span class="p">,</span> <span class="s2">"demo.mp4"</span><span class="p">,</span> <span class="s2">"notes.txt"</span><span class="p">]</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <blockquote class="note"> <p>Supported video formats include .mp4, .mov, .avi, .webm, and others (provider-dependent).</p> <p>Only Google Gemini and VertexAI models currently support video input.</p> <p>Large video files may be subject to size or duration limits imposed by the provider.</p> </blockquote> <p>RubyLLM automatically handles image encoding and formatting for each provider’s API. Local images are read and encoded as needed, while URLs are passed directly when supported by the provider.</p> <h3 id="working-with-audio"> <a href="index.html#working-with-audio" class="anchor-heading" aria-labelledby="working-with-audio"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Working with Audio </h3> <p>Audio-capable models can transcribe speech, analyze audio content, and answer questions about what they hear. Currently, models like <code class="language-plaintext highlighter-rouge">gpt-4o-audio-preview</code> and Google’s <code class="language-plaintext highlighter-rouge">gemini-2.5</code> series of models support audio input.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gpt-4o-audio-preview'</span><span class="p">)</span> <span class="c1"># Use an audio-capable model</span>

<span class="c1"># Transcribe or ask questions about audio content</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Please transcribe this meeting recording."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/meeting.mp3"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Ask follow-up questions based on the audio context</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What were the main action items discussed?"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Gemini example</span>
<span class="n">gemini_chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gemini-2.5-flash'</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">gemini_chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Summarize this podcast."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/podcast.mp3"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <h3 id="working-with-text-files"> <a href="index.html#working-with-text-files" class="anchor-heading" aria-labelledby="working-with-text-files"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Working with Text Files </h3> <p>You can provide text files directly to models for analysis, summarization, or question answering. This works with any text-based format including plain text, code files, CSV, JSON, and more.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>

<span class="c1"># Analyze a text file</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Summarize the key points in this document."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/document.txt"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Ask questions about code files</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Explain what this Ruby file does."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"app/models/user.rb"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <h3 id="working-with-pdfs"> <a href="index.html#working-with-pdfs" class="anchor-heading" aria-labelledby="working-with-pdfs"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Working with PDFs </h3> <p>PDF support allows models to analyze complex documents including reports, manuals, and research papers. Currently, Claude 3+ and Gemini models offer the best PDF support.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Use a model that supports PDFs</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>

<span class="c1"># Ask about a local PDF</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Summarize the key findings in this research paper."</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/paper.pdf"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Ask about a PDF via URL</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What are the terms and conditions outlined here?"</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"https://example.com/terms.pdf"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Combine text and PDF context</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Based on section 3 of this document, what is the warranty period?"</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"manual.pdf"</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <blockquote class="note"> <p>Be mindful of provider-specific limits. For example, Anthropic Claude models currently have a 10MB per-file size limit, and the total size/token count of all PDFs must fit within the model’s context window (e.g., 200,000 tokens for Claude 3 models).</p> </blockquote> <h3 id="automatic-file-type-detection"> <a href="index.html#automatic-file-type-detection" class="anchor-heading" aria-labelledby="automatic-file-type-detection"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Automatic File Type Detection </h3> <p>RubyLLM automatically detects file types based on extensions and content, so you can pass files directly without specifying the type:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>

<span class="c1"># Single file - type automatically detected</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What's in this file?"</span><span class="p">,</span> <span class="ss">with: </span><span class="s2">"path/to/document.pdf"</span>

<span class="c1"># Multiple files of different types</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Analyze these files"</span><span class="p">,</span> <span class="ss">with: </span><span class="p">[</span>
  <span class="s2">"diagram.png"</span><span class="p">,</span>
  <span class="s2">"report.pdf"</span><span class="p">,</span>
  <span class="s2">"meeting_notes.txt"</span><span class="p">,</span>
  <span class="s2">"recording.mp3"</span>
<span class="p">]</span>

<span class="c1"># Still works with the explicit hash format if needed</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What's in this image?"</span><span class="p">,</span> <span class="ss">with: </span><span class="p">{</span> <span class="ss">image: </span><span class="s2">"photo.jpg"</span> <span class="p">}</span>
</code></pre></div></div> <p><strong>Supported file types:</strong></p> <ul> <li><strong>Images:</strong> .jpg, .jpeg, .png, .gif, .webp, .bmp</li> <li><strong>Videos:</strong> .mp4, .mov, .avi, .webm</li> <li><strong>Audio:</strong> .mp3, .wav, .m4a, .ogg, .flac</li> <li><strong>Documents:</strong> .pdf, .txt, .md, .csv, .json, .xml</li> <li><strong>Code:</strong> .rb, .py, .js, .html, .css (and many others)</li> </ul> <h2 id="controlling-response-behavior"> <a href="index.html#controlling-response-behavior" class="anchor-heading" aria-labelledby="controlling-response-behavior"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Controlling Response Behavior </h2> <h3 id="temperature-and-creativity"> <a href="index.html#temperature-and-creativity" class="anchor-heading" aria-labelledby="temperature-and-creativity"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Temperature and Creativity </h3> <p>The temperature parameter controls the randomness of the model’s responses. Understanding temperature helps you get the right balance between creativity and consistency for your use case.</p> <ul> <li><strong>Low temperature (0.0 - 0.3)</strong>: More deterministic and focused responses. Use for factual queries, technical explanations, or when consistency is important.</li> <li><strong>Medium temperature (0.4 - 0.7)</strong>: Balanced creativity and coherence. Good for general conversation and most applications.</li> <li><strong>High temperature (0.8 - 1.0)</strong>: More creative and varied responses. Use for brainstorming, creative writing, or when you want diverse outputs.</li> </ul> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create a chat with low temperature for factual answers</span>
<span class="n">factual_chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">.</span><span class="nf">with_temperature</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
<span class="n">response1</span> <span class="o">=</span> <span class="n">factual_chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What is the boiling point of water at sea level in Celsius?"</span>
<span class="nb">puts</span> <span class="n">response1</span><span class="p">.</span><span class="nf">content</span>

<span class="c1"># Create a chat with high temperature for creative writing</span>
<span class="n">creative_chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">.</span><span class="nf">with_temperature</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)</span>
<span class="n">response2</span> <span class="o">=</span> <span class="n">creative_chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Write a short poem about the color blue."</span>
<span class="nb">puts</span> <span class="n">response2</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">with_temperature</code> method returns the chat instance, allowing you to chain multiple configuration calls together.</p> <h3 id="provider-specific-parameters"> <a href="index.html#provider-specific-parameters" class="anchor-heading" aria-labelledby="provider-specific-parameters"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Provider-Specific Parameters </h3> <p>Different providers offer unique features and parameters. The <code class="language-plaintext highlighter-rouge">with_params</code> method lets you access these provider-specific capabilities while maintaining RubyLLM’s unified interface. Parameters passed via <code class="language-plaintext highlighter-rouge">with_params</code> will override any defaults set by RubyLLM, giving you full control over the API request payload.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># response_format parameter is supported by :openai, :ollama, :deepseek</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">.</span><span class="nf">with_params</span><span class="p">(</span><span class="ss">response_format: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'json_object'</span> <span class="p">})</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What is the square root of 64? Answer with a JSON object with the key `result`."</span>
<span class="nb">puts</span> <span class="no">JSON</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">content</span><span class="p">)</span>
</code></pre></div></div> <blockquote class="warning"> <p><strong>With great power comes great responsibility:</strong> The <code class="language-plaintext highlighter-rouge">with_params</code> method can override any part of the request payload, including critical parameters like model, max_tokens, or tools. Use it carefully to avoid unintended behavior. Always verify that your overrides are compatible with the provider’s API. To debug and see the exact request being sent, set the environment variable <code class="language-plaintext highlighter-rouge">RUBYLLM_DEBUG=true</code>.</p> </blockquote> <blockquote class="warning"> <p>Available parameters vary by provider and model. Always consult the provider’s documentation for supported features. RubyLLM passes these parameters through without validation, so incorrect parameters may cause API errors. Parameters from <code class="language-plaintext highlighter-rouge">with_params</code> take precedence over RubyLLM’s defaults, allowing you to override any aspect of the request payload.</p> </blockquote> <h2 class="d-inline-block" id="raw-content-blocks"> <a href="index.html#raw-content-blocks" class="anchor-heading" aria-labelledby="raw-content-blocks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Raw Content Blocks </h2> <p class="label label-green">v1.9.0+</p> <p>Most of the time you can rely on RubyLLM to format messages for each provider. When you need to send a custom payload as content, wrap it in <code class="language-plaintext highlighter-rouge">RubyLLM::Content::Raw</code>. The block is forwarded verbatim, with no additional processing.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw_block</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Content</span><span class="o">::</span><span class="no">Raw</span><span class="p">.</span><span class="nf">new</span><span class="p">([</span>
  <span class="p">{</span> <span class="ss">type: </span><span class="s1">'text'</span><span class="p">,</span> <span class="ss">text: </span><span class="s1">'Reusable analysis prompt'</span> <span class="p">},</span>
  <span class="p">{</span> <span class="ss">type: </span><span class="s1">'text'</span><span class="p">,</span> <span class="ss">text: </span><span class="s2">"Today's request: </span><span class="si">#{</span><span class="n">summary</span><span class="si">}</span><span class="s2">"</span> <span class="p">}</span>
<span class="p">])</span>

<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="ss">role: :system</span><span class="p">,</span> <span class="ss">content: </span><span class="n">raw_block</span><span class="p">)</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="n">raw_block</span><span class="p">)</span>
</code></pre></div></div> <p>Use raw blocks sparingly: they bypass cross-provider safeguards, so it is your responsibility to ensure the payload matches the provider’s expectations. <code class="language-plaintext highlighter-rouge">Chat#ask</code>, <code class="language-plaintext highlighter-rouge">Chat#add_message</code>, tool results, and streaming accumulators all understand <code class="language-plaintext highlighter-rouge">Content::Raw</code> values.</p> <h3 class="d-inline-block" id="anthropic-prompt-caching"> <a href="index.html#anthropic-prompt-caching" class="anchor-heading" aria-labelledby="anthropic-prompt-caching"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Anthropic Prompt Caching </h3> <p class="label label-green">v1.9.0+</p> <p>One use case for Raw Content Blocks is Anthropic Prompt Caching.</p> <p>Anthropic lets you mark individual prompt blocks for caching, which can dramatically reduce costs on long conversations. RubyLLM provides a convenience builder that returns a <code class="language-plaintext highlighter-rouge">Content::Raw</code> instance with the proper structure:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">system_block</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Providers</span><span class="o">::</span><span class="no">Anthropic</span><span class="o">::</span><span class="no">Content</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span>
  <span class="s2">"You are a release-notes assistant. Always group changes by subsystem."</span><span class="p">,</span>
  <span class="ss">cache: </span><span class="kp">true</span> <span class="c1"># shorthand for cache_control: { type: 'ephemeral' }</span>
<span class="p">)</span>

<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">add_message</span><span class="p">(</span><span class="ss">role: :system</span><span class="p">,</span> <span class="ss">content: </span><span class="n">system_block</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span>
  <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Providers</span><span class="o">::</span><span class="no">Anthropic</span><span class="o">::</span><span class="no">Content</span><span class="p">.</span><span class="nf">new</span><span class="p">(</span>
    <span class="s2">"Summarize the API changes in this diff."</span><span class="p">,</span>
    <span class="ss">cache_control: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'ephemeral'</span><span class="p">,</span> <span class="ss">ttl: </span><span class="s1">'1h'</span> <span class="p">}</span>
  <span class="p">)</span>
<span class="p">)</span>
</code></pre></div></div> <p>Need something even more custom? Build the payload manually and wrap it in <code class="language-plaintext highlighter-rouge">Content::Raw</code>:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">raw_prompt</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Content</span><span class="o">::</span><span class="no">Raw</span><span class="p">.</span><span class="nf">new</span><span class="p">([</span>
  <span class="p">{</span> <span class="ss">type: </span><span class="s1">'text'</span><span class="p">,</span> <span class="ss">text: </span><span class="no">File</span><span class="p">.</span><span class="nf">read</span><span class="p">(</span><span class="s1">'/a/large/file'</span><span class="p">),</span> <span class="ss">cache_control: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'ephemeral'</span> <span class="p">}</span> <span class="p">},</span>
  <span class="p">{</span> <span class="ss">type: </span><span class="s1">'text'</span><span class="p">,</span> <span class="ss">text: </span><span class="s2">"Today's request: </span><span class="si">#{</span><span class="n">summary</span><span class="si">}</span><span class="s2">"</span> <span class="p">}</span>
<span class="p">])</span>

<span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="n">raw_prompt</span><span class="p">)</span>
</code></pre></div></div> <p>The same idea applies to tool definitions:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ChangelogTool</span> <span class="o">&lt;</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Tool</span>
  <span class="n">description</span> <span class="s2">"Formats commits into human-readable changelog entries."</span>
  <span class="n">param</span> <span class="ss">:commits</span><span class="p">,</span> <span class="ss">type: :array</span><span class="p">,</span> <span class="ss">desc: </span><span class="s2">"List of commits to summarize"</span>

  <span class="n">with_params</span> <span class="ss">cache_control: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'ephemeral'</span> <span class="p">}</span>

  <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="n">commits</span><span class="p">:)</span>
    <span class="c1"># ...</span>
  <span class="k">end</span>
<span class="k">end</span>
</code></pre></div></div> <p>Providers that do not understand these extra fields silently ignore them, so you can reuse the same tools across models. See the <a href="../tools/index.html#provider-specific-parameters">Tool Provider Parameters</a> section for more detail.</p> <h3 id="custom-http-headers"> <a href="index.html#custom-http-headers" class="anchor-heading" aria-labelledby="custom-http-headers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Custom HTTP Headers </h3> <p>Some providers offer beta features or special capabilities through custom HTTP headers. The <code class="language-plaintext highlighter-rouge">with_headers</code> method lets you add these headers to your API requests while maintaining RubyLLM’s security model.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Enable Anthropic's beta features</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'claude-sonnet-4'</span><span class="p">)</span>
      <span class="p">.</span><span class="nf">with_headers</span><span class="p">(</span><span class="s1">'anthropic-beta'</span> <span class="o">=&gt;</span> <span class="s1">'fine-grained-tool-streaming-2025-05-14'</span><span class="p">)</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Tell me about the weather"</span>
</code></pre></div></div> <p>Headers are merged with provider defaults, with provider headers taking precedence for security. This means you can’t override authentication or critical headers, but you can add supplementary headers for optional features.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Chain with other configuration methods</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>
      <span class="p">.</span><span class="nf">with_temperature</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span>
      <span class="p">.</span><span class="nf">with_headers</span><span class="p">(</span><span class="s1">'X-Custom-Feature'</span> <span class="o">=&gt;</span> <span class="s1">'enabled'</span><span class="p">)</span>
      <span class="p">.</span><span class="nf">with_params</span><span class="p">(</span><span class="ss">max_tokens: </span><span class="mi">1000</span><span class="p">)</span>
</code></pre></div></div> <blockquote class="warning"> <p>Use custom headers with caution. They may enable experimental features that could change or be removed without notice. Always refer to your provider’s documentation for supported headers and their behavior.</p> </blockquote> <h2 id="getting-structured-output"> <a href="index.html#getting-structured-output" class="anchor-heading" aria-labelledby="getting-structured-output"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Getting Structured Output </h2> <p>When building applications, you often need AI responses in a specific format for parsing and processing. RubyLLM provides two approaches: JSON mode for valid JSON output, and structured output for guaranteed schema compliance.</p> <blockquote class="note"> <p>JSON mode (using <code class="language-plaintext highlighter-rouge">with_params(response_format: { type: 'json_object' })</code>) guarantees valid JSON but not any specific structure. Structured output (<code class="language-plaintext highlighter-rouge">with_schema</code>) guarantees the response matches your exact schema with required fields and types. Use structured output when you need predictable, validated responses.</p> </blockquote> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># JSON mode - guarantees valid JSON, but no specific structure</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">.</span><span class="nf">with_params</span><span class="p">(</span><span class="ss">response_format: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'json_object'</span> <span class="p">})</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"List 3 programming languages with their year created. Return as JSON."</span><span class="p">)</span>
<span class="c1"># Could return any valid JSON structure</span>

<span class="c1"># Structured output - guarantees exact schema</span>
<span class="k">class</span> <span class="nc">LanguagesSchema</span> <span class="o">&lt;</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Schema</span>
  <span class="n">array</span> <span class="ss">:languages</span> <span class="k">do</span>
    <span class="n">object</span> <span class="k">do</span>
      <span class="n">string</span> <span class="ss">:name</span>
      <span class="n">integer</span> <span class="ss">:year</span>
    <span class="k">end</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="no">LanguagesSchema</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"List 3 programming languages with their year created"</span><span class="p">)</span>
<span class="c1"># Always returns: {"languages" =&gt; [{"name" =&gt; "...", "year" =&gt; ...}, ...]}</span>
</code></pre></div></div> <h3 id="using-rubyllmschema-recommended"> <a href="index.html#using-rubyllmschema-recommended" class="anchor-heading" aria-labelledby="using-rubyllmschema-recommended"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Using RubyLLM::Schema (Recommended) </h3> <p>The easiest way to define schemas is with the <a href="https://github.com/danielfriis/ruby_llm-schema">RubyLLM::Schema</a> gem:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># First, add to your Gemfile:</span>
<span class="c1"># gem 'ruby_llm-schema'</span>
<span class="c1">#</span>
<span class="c1"># Then in your code:</span>
<span class="nb">require</span> <span class="s1">'ruby_llm/schema'</span>

<span class="c1"># Define your schema as a class</span>
<span class="k">class</span> <span class="nc">PersonSchema</span> <span class="o">&lt;</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Schema</span>
  <span class="n">string</span> <span class="ss">:name</span><span class="p">,</span> <span class="ss">description: </span><span class="s2">"Person's full name"</span>
  <span class="n">integer</span> <span class="ss">:age</span><span class="p">,</span> <span class="ss">description: </span><span class="s2">"Person's age in years"</span>
  <span class="n">string</span> <span class="ss">:city</span><span class="p">,</span> <span class="ss">required: </span><span class="kp">false</span><span class="p">,</span> <span class="ss">description: </span><span class="s2">"City where they live"</span>
<span class="k">end</span>

<span class="c1"># Use it with a chat</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="no">PersonSchema</span><span class="p">).</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"Generate a person named Alice who is 30 years old"</span><span class="p">)</span>

<span class="c1"># The response is automatically parsed from JSON</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span> <span class="c1"># =&gt; {"name" =&gt; "Alice", "age" =&gt; 30}</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span><span class="p">.</span><span class="nf">class</span> <span class="c1"># =&gt; Hash</span>
</code></pre></div></div> <h3 id="using-manual-json-schemas"> <a href="index.html#using-manual-json-schemas" class="anchor-heading" aria-labelledby="using-manual-json-schemas"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Using Manual JSON Schemas </h3> <p>If you prefer not to use RubyLLM::Schema, you can provide a JSON Schema directly:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">person_schema</span> <span class="o">=</span> <span class="p">{</span>
  <span class="ss">type: </span><span class="s1">'object'</span><span class="p">,</span>
  <span class="ss">properties: </span><span class="p">{</span>
    <span class="ss">name: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'string'</span> <span class="p">},</span>
    <span class="ss">age: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'integer'</span> <span class="p">},</span>
    <span class="ss">hobbies: </span><span class="p">{</span>
      <span class="ss">type: </span><span class="s1">'array'</span><span class="p">,</span>
      <span class="ss">items: </span><span class="p">{</span> <span class="ss">type: </span><span class="s1">'string'</span> <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">},</span>
  <span class="ss">required: </span><span class="p">[</span><span class="s1">'name'</span><span class="p">,</span> <span class="s1">'age'</span><span class="p">,</span> <span class="s1">'hobbies'</span><span class="p">],</span>
  <span class="ss">additionalProperties: </span><span class="kp">false</span>  <span class="c1"># Required for OpenAI structured output</span>
<span class="p">}</span>

<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="n">person_schema</span><span class="p">).</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"Generate a person who likes Ruby"</span><span class="p">)</span>

<span class="c1"># Response is automatically parsed</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">content</span>
<span class="c1"># =&gt; {"name" =&gt; "Bob", "age" =&gt; 25, "hobbies" =&gt; ["Ruby programming", "Open source"]}</span>
</code></pre></div></div> <blockquote class="warning"> <p><strong>OpenAI Requirement:</strong> When using manual JSON schemas with OpenAI, you must include <code class="language-plaintext highlighter-rouge">additionalProperties: false</code> in your schema objects. RubyLLM::Schema handles this automatically.</p> </blockquote> <h3 id="complex-nested-schemas"> <a href="index.html#complex-nested-schemas" class="anchor-heading" aria-labelledby="complex-nested-schemas"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Complex Nested Schemas </h3> <p>Structured output supports complex nested objects and arrays:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CompanySchema</span> <span class="o">&lt;</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Schema</span>
  <span class="n">string</span> <span class="ss">:name</span><span class="p">,</span> <span class="ss">description: </span><span class="s2">"Company name"</span>

  <span class="n">array</span> <span class="ss">:employees</span> <span class="k">do</span>
    <span class="n">object</span> <span class="k">do</span>
      <span class="n">string</span> <span class="ss">:name</span>
      <span class="n">string</span> <span class="ss">:role</span><span class="p">,</span> <span class="ss">enum: </span><span class="p">[</span><span class="s2">"developer"</span><span class="p">,</span> <span class="s2">"designer"</span><span class="p">,</span> <span class="s2">"manager"</span><span class="p">]</span>
      <span class="n">array</span> <span class="ss">:skills</span><span class="p">,</span> <span class="ss">of: :string</span>
    <span class="k">end</span>
  <span class="k">end</span>

  <span class="n">object</span> <span class="ss">:metadata</span> <span class="k">do</span>
    <span class="n">integer</span> <span class="ss">:founded</span>
    <span class="n">string</span> <span class="ss">:industry</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="no">CompanySchema</span><span class="p">).</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"Generate a small tech startup"</span><span class="p">)</span>

<span class="c1"># Access nested data</span>
<span class="n">response</span><span class="p">.</span><span class="nf">content</span><span class="p">[</span><span class="s2">"employees"</span><span class="p">].</span><span class="nf">each</span> <span class="k">do</span> <span class="o">|</span><span class="n">employee</span><span class="o">|</span>
  <span class="nb">puts</span> <span class="s2">"</span><span class="si">#{</span><span class="n">employee</span><span class="p">[</span><span class="s1">'name'</span><span class="p">]</span><span class="si">}</span><span class="s2"> - </span><span class="si">#{</span><span class="n">employee</span><span class="p">[</span><span class="s1">'role'</span><span class="p">]</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>
</code></pre></div></div> <h3 id="provider-support"> <a href="index.html#provider-support" class="anchor-heading" aria-labelledby="provider-support"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Provider Support </h3> <p>Not all models support structured output. Currently supported:</p> <ul> <li><strong>OpenAI</strong>: GPT-4o, GPT-4o-mini, and newer models</li> <li><strong>Anthropic</strong>: No native structured output support. You can simulate it with tool definitions or careful prompting</li> <li><strong>Gemini</strong>: Gemini 1.5 Pro/Flash and newer</li> </ul> <p>Models that don’t support structured output:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span><span class="p">(</span><span class="ss">model: </span><span class="s1">'gpt-3.5-turbo'</span><span class="p">)</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s1">'Generate a person'</span><span class="p">)</span>
<span class="c1"># Provider will return an error if unsupported</span>
</code></pre></div></div> <h3 id="multi-turn-conversations-with-schemas"> <a href="index.html#multi-turn-conversations-with-schemas" class="anchor-heading" aria-labelledby="multi-turn-conversations-with-schemas"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Multi-turn Conversations with Schemas </h3> <p>You can add or remove schemas during a conversation:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Start with a schema</span>
<span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="no">PersonSchema</span><span class="p">)</span>
<span class="n">person</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"Generate a person"</span><span class="p">)</span>

<span class="c1"># Remove the schema for free-form responses</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="kp">nil</span><span class="p">)</span>
<span class="n">analysis</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"Tell me about this person's potential career paths"</span><span class="p">)</span>

<span class="c1"># Add a different schema</span>
<span class="k">class</span> <span class="nc">CareerPlanSchema</span> <span class="o">&lt;</span> <span class="no">RubyLLM</span><span class="o">::</span><span class="no">Schema</span>
  <span class="n">string</span> <span class="ss">:title</span>
  <span class="n">array</span> <span class="ss">:steps</span><span class="p">,</span> <span class="ss">of: :string</span>
  <span class="n">integer</span> <span class="ss">:years_required</span>
<span class="k">end</span>

<span class="n">chat</span><span class="p">.</span><span class="nf">with_schema</span><span class="p">(</span><span class="no">CareerPlanSchema</span><span class="p">)</span>
<span class="n">career</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"Now structure a career plan"</span><span class="p">)</span>

<span class="nb">puts</span> <span class="n">person</span><span class="p">.</span><span class="nf">content</span>
<span class="nb">puts</span> <span class="n">analysis</span><span class="p">.</span><span class="nf">content</span>
<span class="nb">puts</span> <span class="n">career</span><span class="p">.</span><span class="nf">content</span>
</code></pre></div></div> <h2 id="tracking-token-usage"> <a href="index.html#tracking-token-usage" class="anchor-heading" aria-labelledby="tracking-token-usage"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Tracking Token Usage </h2> <p>Understanding token usage is important for managing costs and staying within context limits. Each <code class="language-plaintext highlighter-rouge">RubyLLM::Message</code> returned by <code class="language-plaintext highlighter-rouge">ask</code> includes token counts.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"Explain the Ruby Global Interpreter Lock (GIL)."</span>

<span class="n">input_tokens</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">input_tokens</span>   <span class="c1"># Tokens in the prompt sent TO the model</span>
<span class="n">output_tokens</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">output_tokens</span> <span class="c1"># Tokens in the response FROM the model</span>
<span class="n">cached_tokens</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">cached_tokens</span> <span class="c1"># Tokens served from the provider's prompt cache (if supported) - v1.9.0+</span>
<span class="n">cache_creation_tokens</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="nf">cache_creation_tokens</span> <span class="c1"># Tokens written to the cache (Anthropic/Bedrock) - v1.9.0+</span>

<span class="nb">puts</span> <span class="s2">"Input Tokens: </span><span class="si">#{</span><span class="n">input_tokens</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">puts</span> <span class="s2">"Output Tokens: </span><span class="si">#{</span><span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span>
<span class="nb">puts</span> <span class="s2">"Cached Prompt Tokens: </span><span class="si">#{</span><span class="n">cached_tokens</span><span class="si">}</span><span class="s2">"</span> <span class="c1"># v1.9.0+</span>
<span class="nb">puts</span> <span class="s2">"Cache Creation Tokens: </span><span class="si">#{</span><span class="n">cache_creation_tokens</span><span class="si">}</span><span class="s2">"</span> <span class="c1"># v1.9.0+</span>
<span class="nb">puts</span> <span class="s2">"Total Tokens for this turn: </span><span class="si">#{</span><span class="n">input_tokens</span> <span class="o">+</span> <span class="n">output_tokens</span><span class="si">}</span><span class="s2">"</span>

<span class="c1"># Estimate cost for this turn</span>
<span class="n">model_info</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">models</span><span class="p">.</span><span class="nf">find</span><span class="p">(</span><span class="n">response</span><span class="p">.</span><span class="nf">model_id</span><span class="p">)</span>
<span class="k">if</span> <span class="n">model_info</span><span class="p">.</span><span class="nf">input_price_per_million</span> <span class="o">&amp;&amp;</span> <span class="n">model_info</span><span class="p">.</span><span class="nf">output_price_per_million</span>
  <span class="n">input_cost</span> <span class="o">=</span> <span class="n">input_tokens</span> <span class="o">*</span> <span class="n">model_info</span><span class="p">.</span><span class="nf">input_price_per_million</span> <span class="o">/</span> <span class="mi">1_000_000</span>
  <span class="n">output_cost</span> <span class="o">=</span> <span class="n">output_tokens</span> <span class="o">*</span> <span class="n">model_info</span><span class="p">.</span><span class="nf">output_price_per_million</span> <span class="o">/</span> <span class="mi">1_000_000</span>
  <span class="n">turn_cost</span> <span class="o">=</span> <span class="n">input_cost</span> <span class="o">+</span> <span class="n">output_cost</span>
  <span class="nb">puts</span> <span class="s2">"Estimated Cost for this turn: $</span><span class="si">#{</span><span class="nb">format</span><span class="p">(</span><span class="s1">'%.6f'</span><span class="p">,</span> <span class="n">turn_cost</span><span class="p">)</span><span class="si">}</span><span class="s2">"</span>
<span class="k">else</span>
  <span class="nb">puts</span> <span class="s2">"Pricing information not available for </span><span class="si">#{</span><span class="n">model_info</span><span class="p">.</span><span class="nf">id</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>

<span class="c1"># Total tokens for the entire conversation so far</span>
<span class="n">total_conversation_tokens</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">messages</span><span class="p">.</span><span class="nf">sum</span> <span class="p">{</span> <span class="o">|</span><span class="n">msg</span><span class="o">|</span> <span class="p">(</span><span class="n">msg</span><span class="p">.</span><span class="nf">input_tokens</span> <span class="o">||</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">msg</span><span class="p">.</span><span class="nf">output_tokens</span> <span class="o">||</span> <span class="mi">0</span><span class="p">)</span> <span class="p">}</span>
<span class="nb">puts</span> <span class="s2">"Total Conversation Tokens: </span><span class="si">#{</span><span class="n">total_conversation_tokens</span><span class="si">}</span><span class="s2">"</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">cached_tokens</code> captures the portion of the prompt served from the provider’s cache. OpenAI reports this value automatically for prompts over 1024 tokens, while Anthropic and Bedrock/Claude expose both cache hits and cache writes. When the provider does not send cache data the attributes remain <code class="language-plaintext highlighter-rouge">nil</code>, so the example above falls back to zero for display. Available from v1.9+</p> <p>Refer to the <a href="../models/index.html">Working with Models Guide</a> for details on accessing model-specific pricing.</p> <h2 id="chat-event-handlers"> <a href="index.html#chat-event-handlers" class="anchor-heading" aria-labelledby="chat-event-handlers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Chat Event Handlers </h2> <p>You can register blocks to be called when certain events occur during the chat lifecycle. This is particularly useful for UI updates, logging, analytics, or building real-time chat interfaces.</p> <h3 id="available-event-handlers"> <a href="index.html#available-event-handlers" class="anchor-heading" aria-labelledby="available-event-handlers"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Available Event Handlers </h3> <p>RubyLLM provides four event handlers that cover the complete chat lifecycle:</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chat</span> <span class="o">=</span> <span class="no">RubyLLM</span><span class="p">.</span><span class="nf">chat</span>

<span class="c1"># Called at first chunk received from the assistant</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">on_new_message</span> <span class="k">do</span>
  <span class="nb">print</span> <span class="s2">"Assistant &gt; "</span>
<span class="k">end</span>

<span class="c1"># Called after the complete assistant message (including tool calls/results) is received</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">on_end_message</span> <span class="k">do</span> <span class="o">|</span><span class="n">message</span><span class="o">|</span>
  <span class="nb">puts</span> <span class="s2">"Response complete!"</span>
  <span class="c1"># Note: message might be nil if an error occurred during the request</span>
  <span class="k">if</span> <span class="n">message</span> <span class="o">&amp;&amp;</span> <span class="n">message</span><span class="p">.</span><span class="nf">output_tokens</span>
    <span class="nb">puts</span> <span class="s2">"Used </span><span class="si">#{</span><span class="n">message</span><span class="p">.</span><span class="nf">input_tokens</span> <span class="o">+</span> <span class="n">message</span><span class="p">.</span><span class="nf">output_tokens</span><span class="si">}</span><span class="s2"> tokens"</span>
  <span class="k">end</span>
<span class="k">end</span>

<span class="c1"># Called when the AI decides to use a tool</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">on_tool_call</span> <span class="k">do</span> <span class="o">|</span><span class="n">tool_call</span><span class="o">|</span>
  <span class="nb">puts</span> <span class="s2">"AI is calling tool: </span><span class="si">#{</span><span class="n">tool_call</span><span class="p">.</span><span class="nf">name</span><span class="si">}</span><span class="s2"> with arguments: </span><span class="si">#{</span><span class="n">tool_call</span><span class="p">.</span><span class="nf">arguments</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>

<span class="c1"># Called after a tool returns its result</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">on_tool_result</span> <span class="k">do</span> <span class="o">|</span><span class="n">result</span><span class="o">|</span>
  <span class="nb">puts</span> <span class="s2">"Tool returned: </span><span class="si">#{</span><span class="n">result</span><span class="si">}</span><span class="s2">"</span>
<span class="k">end</span>

<span class="c1"># These callbacks work for both streaming and non-streaming requests</span>
<span class="n">chat</span><span class="p">.</span><span class="nf">ask</span> <span class="s2">"What is metaprogramming in Ruby?"</span>
</code></pre></div></div> <h2 id="raw-responses"> <a href="index.html#raw-responses" class="anchor-heading" aria-labelledby="raw-responses"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Raw Responses </h2> <p>You can access the raw response from the API provider with <code class="language-plaintext highlighter-rouge">response.raw</code>.</p> <div class="language-ruby highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">chat</span><span class="p">.</span><span class="nf">ask</span><span class="p">(</span><span class="s2">"What is the capital of France?"</span><span class="p">)</span>
<span class="nb">puts</span> <span class="n">response</span><span class="p">.</span><span class="nf">raw</span><span class="p">.</span><span class="nf">body</span>
</code></pre></div></div> <p>The raw response is a <code class="language-plaintext highlighter-rouge">Faraday::Response</code> object, which you can use to access the headers, body, and status code.</p> <h2 id="next-steps"> <a href="index.html#next-steps" class="anchor-heading" aria-labelledby="next-steps"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Next Steps </h2> <p>This guide covered the core <code class="language-plaintext highlighter-rouge">Chat</code> interface. Now you might want to explore:</p> <ul> <li><a href="../models/index.html">Working with Models</a>: Learn how to choose the best model and handle custom endpoints.</li> <li><a href="../tools/index.html">Using Tools</a>: Enable the AI to call your Ruby code.</li> <li><a href="../streaming/index.html">Streaming Responses</a>: Get real-time feedback from the AI.</li> <li><a href="../rails/index.html">Rails Integration</a>: Persist your chat conversations easily.</li> <li><a href="../error-handling/index.html">Error Handling</a>: Build robust applications that handle API issues.</li> </ul> </main> <hr> <footer> Brought to you by <a href="https://paolino.me">Carmine Paolino</a>, maker of <a href="https://chatwithwork.com"><img src="https://chatwithwork.com/logotype.svg" alt="Chat with Work" class="chatwithwork-logo" style="height: 1.8em; vertical-align: middle; margin: 0 0.25em;"></a> <em style="font-size: 0.85em; opacity: 0.8;">— Claude Code for your documents</em> <div class="d-flex mt-2"> </div> </footer> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
