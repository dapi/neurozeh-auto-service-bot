# frozen_string_literal: true

require 'ruby_llm'
require 'logger'
require_relative 'request_detector'

class LLMClient
  MAX_RETRIES = 1

  def initialize(config, logger = Logger.new($stdout))
    @config = config
    @logger = logger
    @logger.info 'LLMClient initialized with system prompt and price list'
  end

  def send_message(messages, user_info = nil)
    @logger.info "=== LLM CLIENT SEND_MESSAGE START ==="
    @logger.info "Sending message to LLM with #{messages.length} messages"
    @logger.info "User info: #{user_info.inspect}"

    # ĞšĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ¸Ñ€ÑƒĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğ¹ Ğ¿Ñ€Ğ¾Ğ¼Ğ¿Ñ‚ Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸ Ğ¸ Ğ¿Ñ€Ğ°Ğ¹Ñ-Ğ»Ğ¸ÑÑ‚Ğ¾Ğ¼
    combined_system_prompt = build_combined_system_prompt

    retries = 0
    begin
      # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ° Ğ² Ğ·Ğ°Ğ²Ğ¸ÑĞ¸Ğ¼Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ‚ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
      @logger.info "LLMClient model: #{@config.llm_model}, provider: #{@config.llm_provider}"
      # Ğ’Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ‡Ğ°Ñ‚: ĞºĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ Ğ¸Ğ»Ğ¸ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ñ‹Ğ¹
      chat = RubyLLM.chat model: @config.llm_model, provider: @config.llm_provider, assume_model_exists: true

      # Ğ£ÑÑ‚Ğ°Ğ½Ğ°Ğ²Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ğµ Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ğ¸
      chat.with_instructions(combined_system_prompt, replace: true)

      # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ RequestDetector tool ĞµÑĞ»Ğ¸ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½ admin_chat_id Ğ¸ ĞµÑÑ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ğµ
      if user_info && @config.admin_chat_id
        # RequestDetector Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¾Ñ‚ AI Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
        request_detector = RequestDetector.new(@config, @logger)
        chat.with_tool(request_detector)

        # Ğ›Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¾Ğ² tool
        chat.on_tool_call do |tool_call|
          @logger.info "ğŸ”” REQUEST DETECTED: AI calling tool: #{tool_call.name} for user #{tool_call.arguments[:user_id]}"
          @logger.debug "Tool arguments: #{tool_call.arguments}"
        end

        chat.on_tool_result do |result|
          if result[:success]
            @logger.info "âœ… REQUEST SENT: #{result[:request_type]} for user #{user_info[:id]}"
          elsif result[:error]
            @logger.error "âŒ REQUEST ERROR: #{result[:error]}"
          else
            @logger.warn "âŒ REQUEST REJECTED: #{result[:reason]}"
          end
        end
      end

      # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ
      last_message = messages.last
      raise ArgumentError, 'No messages to send' unless last_message
      raise ArgumentError, 'Last message is not from user' unless last_message[:role] == 'user'

      @logger.debug "Last message content: #{last_message[:content][0..100]}..."

      # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ° Ğ² ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ´Ğ»Ñ AI, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ Ğ¾Ğ½ Ğ¼Ğ¾Ğ³ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ RequestDetector Ñ Ğ½ÑƒĞ¶Ğ½Ñ‹Ğ¼Ğ¸ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ğ°Ğ¼Ğ¸
      if user_info && @config.admin_chat_id && messages.length > 1
        context_messages = messages[0..-2]
        if context_messages.any?
          conversation_context = context_messages.map { |msg| "#{msg[:role]}: #{msg[:content]}" }.join("\n\n")
          # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ¾ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ñ, Ñ‡Ñ‚Ğ¾Ğ±Ñ‹ AI Ğ¼Ğ¾Ğ³ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ²ÑÑ Ğ±ĞµÑĞµĞ´Ñƒ
          contextual_message = "ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¸Ğ°Ğ»Ğ¾Ğ³Ğ°:\n#{conversation_context}\n\nĞ¢ĞµĞºÑƒÑ‰ĞµĞµ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»Ñ: #{last_message[:content]}"
          last_message = { role: 'user', content: contextual_message }
          @logger.debug "Enhanced message with conversation context for RequestDetector"
        end
      end

      # ĞÑ‚Ğ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚
      @logger.info "=== SENDING TO RUBYLLM API ==="
      @logger.info "Using provider: #{@config.llm_provider}, model: #{@config.llm_model}"
      @logger.info "Last message: #{last_message[:content][0..100]}..."
      @logger.debug "Full last message: #{last_message.inspect}"

      response = chat.ask(last_message[:content])

      @logger.info "=== RECEIVED RESPONSE FROM RUBYLLM API ==="
      @logger.info "Response type: #{response.class}"
      @logger.info "Response content length: #{response.content&.length || 0}"
      @logger.debug "Response object: #{response.inspect}"

      # Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑÑ‚ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°
      response.content
    rescue RubyLLM::ConfigurationError => e
      @logger.error "RubyLLM configuration error: #{e.message}"
      raise e
    rescue RubyLLM::ModelNotFoundError => e
      @logger.error "Model not found error: #{e.message}"
      raise e
    rescue RubyLLM::Error => e
      @logger.error "RubyLLM API error: #{e.message}"
      raise e
    rescue StandardError => e
      retries += 1
      if retries <= MAX_RETRIES
        @logger.warn "Error sending message to RubyLLM, retrying (#{retries}/#{MAX_RETRIES}): #{e.message}"
        @logger.warn "Error class: #{e.class}"
        @logger.warn "Error backtrace: #{e.backtrace&.first(5)&.join(', ')}"
        sleep(1) # Wait before retrying
        retry
      else
        @logger.error "Failed to send message to RubyLLM after #{MAX_RETRIES} retries: #{e.message}"
        @logger.error "Final error class: #{e.class}"
        @logger.error "Final error backtrace: #{e.backtrace&.first(10)&.join("\n")}"
        raise e
      end
    end
  end

  private

  def build_combined_system_prompt
    # Ğ—Ğ°Ğ¼ĞµĞ½ÑĞµĞ¼ Ğ¿Ğ»ĞµĞ¹ÑÑ…Ğ¾Ğ»Ğ´ĞµÑ€ [COMPANY_INFO] Ğ½Ğ° ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ğ¼Ğ¾Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ¾ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸
    prompt_with_company = @config.system_prompt.gsub('[COMPANY_INFO]', @config.company_info)

    # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¿Ñ€Ğ°Ğ¹Ñ-Ğ»Ğ¸ÑÑ‚
    "#{prompt_with_company}\n\n---\n\n## ĞŸĞ ĞĞ™Ğ¡-Ğ›Ğ˜Ğ¡Ğ¢\n\n#{@config.formatted_price_list}"
  end
end
